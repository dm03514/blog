# Debugging your DevOps Using ValueStream and LightStep - Root Cause Analysis
practices of DevOps are accelerating with the advent of continuous delivery. DORA, and cloudnative foundation.  The analysis of DevOps success and efficacy is still dark arts among senior leaders.  ValueStream brings DevOps monitoring up to CloudNative speeds.  It allows This post shows how ValueStream can be used to help decision makers debug their delivery pipelines.  

Consider you're working for an organization and you would like to ship faster? How would you go about doing it? How do you map visualize and begin to understand the delivery process of each team in your org? How do you start to analyze candidates for improvement? Where do you begin to focus your efforts?

This has traditionally been a difficult process. Just building a mental model of delivery for a single team (let alone multiple teams) may involved consulting multiple different systems for metrics, interviews, embedding, inferring metrics through proxies, relying on experience or just plain guessing and experimentation.  ValueStream, backed by LightStep, completely changes this paradigm.  Delivery metrics are stored in a single system providing a uniform view into performance.  LightStep provides advanced debugging features of latency.  This article walks through how ValueStream and LightStep can help to quickly provide decision makers and with delivery root cause hypothesis.


This article relies on data generated by ValueStream (backed by LightStep) generated through ValueStreams Github and Jenkins integration.  This will look like:

Some background on this example.

There are 3 teams in the organization, using 3 languages: python, go and javascript.  Additionally, there are 2 deployment types, container, mutable and baked ami.  

---
NOTE: The data in the examples below was manually generated by submitting github annd jenkins events to ValueStream. The data is completely valid and representative of real data with the sole EXCEPTION of the event durations.  Since this was automatically generated the event duration in this tutorial are seconds.  In real life they would most likely be in hours or days.
---
# Issue latency (Lead Time)

ValueStream and LightStep offer a birds eye view of an organizations software delivery system.  This supports easiliy seeing overall work lead time:

<<<IMAGE OF OVERALL LATENCY>>

and then drilling down by tag

<<<AGGREGATES BY TAG (team)>>>

While this give a top level view and are expected of any metric system LightStep really starts to shine for debugging.  LighsStep correlations (a built in LightStep feature) is able to analyze what attributes latent operations (issues) have in common:

Longest issues are focused on mutable deployment types in python.

In the example above shows that the longest issues are much more likely to have mutable deployment types and involve python!

Imagine how long building a technical inventory and understanding all languages and delivery methods and teams would take.  ValueStream and opentracing (LightStep) has all this information, automatically collected, automatically analyzable in single place.

# Deploy Failure Practices
The next item that will be analyzed are Deployments.  Correlations are able to automatically analyze which properites are more likely to result in slow ad failed deployments:

<<<DEPLOYMENT FAILURE CORRELATION>>>
Deployment failures are most associated with mutable deployment type.


This post uses LightStep as a ValueStream datastore in order to show

https://medium.com/@dm03514/valuestream-devops-metrics-observing-delivery-across-multiple-systems-7ae76a6e8deb

https://github.com/ImpactInsights/valuestream
