# Debugging your DevOps Using ValueStream and LightStep - Root Cause Analysis
practices of DevOps are accelerating with the advent of continuous delivery. DORA, and cloudnative foundation.  The analysis of DevOps success and efficacy is still dark arts among senior leaders.  ValueStream brings DevOps monitoring up to CloudNative speeds.  It allows This post shows how ValueStream can be used to help decision makers debug their delivery pipelines.  

Consider you're working for an organization and you would like to ship faster? How would you go about doing it? How do you map visualize and begin to understand the delivery process of each team in your org? How do you start to analyze candidates for improvement? Where do you begin to focus your efforts?

This has traditionally been a difficult process. Just building a mental model of delivery for a single team (let alone multiple teams) may involved consulting multiple different systems for metrics, interviews, embedding, inferring metrics through proxies, relying on experience or just plain guessing and experimentation.  ValueStream, backed by LightStep, completely changes this paradigm.  Delivery metrics are stored in a single system providing a uniform view into performance.  LightStep provides advanced debugging features of latency.  This article walks through how ValueStream and LightStep can help to quickly provide decision makers and with delivery root cause hypothesis.


This article relies on data generated by ValueStream (backed by LightStep) generated through ValueStreams Github and Jenkins integration.  This will look like:

Some background on this example.

There are 3 teams in the test organization.  Additionally, there are 2 deployment types, container, mutable and baked ami.  

---
NOTE: The data in the examples below was manually generated by submitting github annd jenkins events to ValueStream. The data is completely valid and representative of real data with the sole EXCEPTION of the event durations.  Since this was automatically generated the event duration in this tutorial are in milliseconds.  In real life they would most likely be in hours or days.
---

# Issue latency (Lead Time)

ValueStream and LightStep offer a birds eye view of an organizations software delivery system.  This supports easiliy seeing overall work lead time:

<p align="center">
  <img src="static/issue_build_relationship.png">
</p>


In the issue above the total lead time is represented by the amount of time an issue is open.


While this give a top level view and are expected of any metric system LightStep really starts to shine for debugging.  LighsStep correlations (a built in LightStep feature) is able to analyze what attributes latent operations (issues) have in common:
  
<p align="center">
  <img src="static/ami_slow_deploys.png">
</p>


In the example above shows that the longest issues are much more likely to have ami based builds and jenkins deploys.  


Since tracing maintains a graph of relationships its able to associate events within a trace. in this case deploys are a build event within a trace:

<p align="center">
  <img src="static/issue_parent.png">
</p>

Tracing is able to take the relationship, and use it to provide context into why some issues take longer to complete then others.  In this case ami based builds are present in the longest issues:

<p align="center">
  <img src="static/ami_weighted_deploys.png">
</p>


Imagine how long building a technical inventory and understanding all languages and delivery methods and teams would take.  ValueStream and opentracing (LightStep) has all this information, automatically collected, automatically analyzable in single place.

Inversely lightstep is also able to dynamically analyze deploys to see that containers were the fastest

<p align="center">
  <img src="static/fast_containers.png">
</p>

# Deploy Failure Practices

The next item that will be analyzed are Deployments.  Correlations are able to automatically analyze which properites are more likely to result in slow ad failed deployments:


<p align="center">
  <img src="static/deploy_debugging_aggregate_by_type.png">
</p>

The aggregate shows that Deployment failures are most associated with mutable deployment type. in traditional aggregtaion based debugging iot's usuallyu up to ann operator to knonw which aggregates and fields are important.  Using ValueStream annd Lightstep lightstep is able to automatically determine that mutable build are responsible for the most latent builds:

Lightstep is also able to automatically identify this based on tags:

<p align="center">
  <img src="static/latent_builds_mutable.png">
</p>


The same automatic analysis can be applied to deploy errors:

<p align="center">
  <img src="static/deploy_errors_mutable_latency.png">
</p>


LightStep is automatically able to determine that failed builds are more likely to occur during "mutable" type deployments.


ValueStream makes capture issue pull request and jenkins build events trivial.  LightStep enables advanced analysis of the build pipeline.  

This post uses LightStep as a ValueStream datastore in order to show

https://medium.com/@dm03514/valuestream-devops-metrics-observing-delivery-across-multiple-systems-7ae76a6e8deb

https://github.com/ImpactInsights/valuestream
